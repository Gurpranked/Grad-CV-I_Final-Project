{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c608b0d",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Vision Transformer Model Sandbox\n",
    "\n",
    "Gurpreet Singh, Logan Sehr, Chinmaykumar Brahmbhatt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cb4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_fea