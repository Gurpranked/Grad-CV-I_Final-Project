{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b676db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "IMAGES_PATH = os.getenv('IMAGES_PATH')\n",
    "ROOT_DATA_PATH = os.getenv('ROOT_DATA_PATH')\n",
    "BATCH_SIZE= int(os.getenv('BATCH_SIZE'))\n",
    "EPOCHS = int(os.getenv('EPOCHS'))\n",
    "LR = float(os.getenv('LR'))\n",
    "PATCH_SIZE=int(os.getenv('PATCH_SIZE'))\n",
    "DROPOUT=float(os.getenv('DROPOUT'))\n",
    "ATTENTION_DROPOUT=float(os.getenv('ATTENTION_DROPOUT'))\n",
    "HIDDEN_DIM=int(os.getenv('HIDDEN_DIM'))\n",
    "MLP_DIM=int(os.getenv('MLP_DIM'))\n",
    "NUM_HEADS=int(os.getenv('NUM_HEADS'))\n",
    "NUM_LAYERS=int(os.getenv('NUM_LAYERS'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d0ac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: 6000\n",
      "Test Set Size: 500\n",
      "Val Set Size: 200\n"
     ]
    }
   ],
   "source": [
    "from preprocess.data_process import get_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afaa1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import VisionTransformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(image_size=80, patch_size=PATCH_SIZE, num_layers=NUM_LAYERS, \n",
    "                                       num_heads=NUM_HEADS, hidden_dim=HIDDEN_DIM, \n",
    "                                       mlp_dim=MLP_DIM, dropout=DROPOUT,\n",
    "                                       attention_dropout=ATTENTION_DROPOUT,\n",
    "                                       num_classes=2)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156e483c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [64, 2]                   768\n",
       "├─Conv2d: 1-1                                 [64, 768, 8, 8]           231,168\n",
       "├─Encoder: 1-2                                [64, 65, 768]             49,920\n",
       "│    └─Dropout: 2-1                           [64, 65, 768]             --\n",
       "│    └─Sequential: 2-2                        [64, 65, 768]             --\n",
       "│    │    └─EncoderBlock: 3-1                 [64, 65, 768]             7,087,872\n",
       "│    │    └─EncoderBlock: 3-2                 [64, 65, 768]             7,087,872\n",
       "│    │    └─EncoderBlock: 3-3                 [64, 65, 768]             7,087,872\n",
       "│    │    └─EncoderBlock: 3-4                 [64, 65, 768]             7,087,872\n",
       "│    │    └─EncoderBlock: 3-5                 [64, 65, 768]             7,087,872\n",
       "│    │    └─EncoderBlock: 3-6                 [64, 65, 768]             7,087,872\n",
       "│    └─LayerNorm: 2-3                         [64, 65, 768]             1,536\n",
       "├─Sequential: 1-3                             [64, 2]                   --\n",
       "│    └─Linear: 2-4                            [64, 2]                   1,538\n",
       "===============================================================================================\n",
       "Total params: 42,812,162\n",
       "Trainable params: 42,812,162\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.76\n",
       "===============================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 1124.21\n",
       "Params size (MB): 114.35\n",
       "Estimated Total Size (MB): 1243.47\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(64, 3, 80, 80), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68c7358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2570, 0.2608, 0.2743,  ..., 0.2534, 0.2729, 0.2812],\n",
       "          [0.2671, 0.2700, 0.2787,  ..., 0.2575, 0.2705, 0.2753],\n",
       "          [0.2811, 0.2847, 0.2871,  ..., 0.2717, 0.2702, 0.2661],\n",
       "          ...,\n",
       "          [0.2559, 0.2574, 0.2608,  ..., 0.2618, 0.2625, 0.2610],\n",
       "          [0.2523, 0.2559, 0.2639,  ..., 0.2640, 0.2601, 0.2580],\n",
       "          [0.2501, 0.2543, 0.2643,  ..., 0.2661, 0.2607, 0.2588]],\n",
       "\n",
       "         [[0.2814, 0.2837, 0.2935,  ..., 0.3048, 0.3189, 0.3235],\n",
       "          [0.2902, 0.2916, 0.2974,  ..., 0.3067, 0.3153, 0.3171],\n",
       "          [0.3020, 0.3034, 0.3043,  ..., 0.3139, 0.3103, 0.3053],\n",
       "          ...,\n",
       "          [0.2902, 0.2913, 0.2945,  ..., 0.2950, 0.2921, 0.2889],\n",
       "          [0.2876, 0.2912, 0.2992,  ..., 0.2961, 0.2893, 0.2860],\n",
       "          [0.2863, 0.2910, 0.3010,  ..., 0.2970, 0.2892, 0.2862]],\n",
       "\n",
       "         [[0.2130, 0.2203, 0.2347,  ..., 0.2275, 0.2381, 0.2420],\n",
       "          [0.2194, 0.2265, 0.2374,  ..., 0.2304, 0.2354, 0.2362],\n",
       "          [0.2273, 0.2345, 0.2405,  ..., 0.2401, 0.2334, 0.2269],\n",
       "          ...,\n",
       "          [0.2143, 0.2168, 0.2226,  ..., 0.2174, 0.2178, 0.2163],\n",
       "          [0.2116, 0.2164, 0.2271,  ..., 0.2170, 0.2130, 0.2110],\n",
       "          [0.2099, 0.2156, 0.2285,  ..., 0.2172, 0.2117, 0.2098]]],\n",
       "\n",
       "\n",
       "        [[[0.6539, 0.6583, 0.6593,  ..., 0.5593, 0.5632, 0.5696],\n",
       "          [0.6544, 0.6596, 0.6603,  ..., 0.5495, 0.5573, 0.5672],\n",
       "          [0.6495, 0.6559, 0.6618,  ..., 0.5429, 0.5554, 0.5681],\n",
       "          ...,\n",
       "          [0.5525, 0.5559, 0.5581,  ..., 0.4701, 0.4853, 0.4853],\n",
       "          [0.5593, 0.5640, 0.5640,  ..., 0.4674, 0.4797, 0.4794],\n",
       "          [0.5647, 0.5711, 0.5716,  ..., 0.4642, 0.4721, 0.4706]],\n",
       "\n",
       "         [[0.6225, 0.6265, 0.6299,  ..., 0.5436, 0.5456, 0.5500],\n",
       "          [0.6225, 0.6272, 0.6306,  ..., 0.5353, 0.5399, 0.5471],\n",
       "          [0.6186, 0.6245, 0.6326,  ..., 0.5297, 0.5373, 0.5466],\n",
       "          ...,\n",
       "          [0.5392, 0.5434, 0.5456,  ..., 0.4520, 0.4627, 0.4613],\n",
       "          [0.5436, 0.5478, 0.5473,  ..., 0.4510, 0.4578, 0.4554],\n",
       "          [0.5471, 0.5520, 0.5515,  ..., 0.4485, 0.4510, 0.4470]],\n",
       "\n",
       "         [[0.5549, 0.5598, 0.5618,  ..., 0.4755, 0.4765, 0.4814],\n",
       "          [0.5569, 0.5623, 0.5635,  ..., 0.4664, 0.4706, 0.4784],\n",
       "          [0.5529, 0.5591, 0.5650,  ..., 0.4600, 0.4674, 0.4775],\n",
       "          ...,\n",
       "          [0.4574, 0.4610, 0.4625,  ..., 0.3762, 0.3875, 0.3873],\n",
       "          [0.4618, 0.4654, 0.4650,  ..., 0.3777, 0.3868, 0.3858],\n",
       "          [0.4637, 0.4686, 0.4691,  ..., 0.3775, 0.3828, 0.3804]]],\n",
       "\n",
       "\n",
       "        [[[0.4119, 0.4150, 0.4201,  ..., 0.4137, 0.4156, 0.3987],\n",
       "          [0.4103, 0.4128, 0.4173,  ..., 0.4259, 0.4263, 0.4133],\n",
       "          [0.4080, 0.4092, 0.4132,  ..., 0.4471, 0.4412, 0.4326],\n",
       "          ...,\n",
       "          [0.5246, 0.4425, 0.3041,  ..., 0.3418, 0.3754, 0.3925],\n",
       "          [0.5172, 0.4373, 0.3015,  ..., 0.3776, 0.4123, 0.4283],\n",
       "          [0.5241, 0.4456, 0.3103,  ..., 0.3972, 0.4326, 0.4474]],\n",
       "\n",
       "         [[0.4261, 0.4293, 0.4349,  ..., 0.3636, 0.3662, 0.3538],\n",
       "          [0.4250, 0.4275, 0.4324,  ..., 0.3709, 0.3737, 0.3642],\n",
       "          [0.4244, 0.4258, 0.4289,  ..., 0.3836, 0.3835, 0.3783],\n",
       "          ...,\n",
       "          [0.4836, 0.4111, 0.2870,  ..., 0.3039, 0.3290, 0.3415],\n",
       "          [0.4768, 0.4063, 0.2842,  ..., 0.3279, 0.3523, 0.3629],\n",
       "          [0.4816, 0.4125, 0.2915,  ..., 0.3415, 0.3651, 0.3738]],\n",
       "\n",
       "         [[0.3452, 0.3496, 0.3586,  ..., 0.2824, 0.2870, 0.2765],\n",
       "          [0.3443, 0.3478, 0.3557,  ..., 0.2899, 0.2952, 0.2870],\n",
       "          [0.3442, 0.3460, 0.3521,  ..., 0.2997, 0.3043, 0.3007],\n",
       "          ...,\n",
       "          [0.4290, 0.3608, 0.2449,  ..., 0.2257, 0.2460, 0.2556],\n",
       "          [0.4213, 0.3550, 0.2420,  ..., 0.2492, 0.2704, 0.2790],\n",
       "          [0.4257, 0.3608, 0.2492,  ..., 0.2625, 0.2830, 0.2900]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.3373, 0.3294, 0.3255,  ..., 0.3098, 0.3255, 0.3412],\n",
       "          [0.3333, 0.3373, 0.3294,  ..., 0.3176, 0.3176, 0.3294],\n",
       "          [0.3373, 0.3451, 0.3490,  ..., 0.3255, 0.3255, 0.3176],\n",
       "          ...,\n",
       "          [0.2941, 0.3020, 0.2941,  ..., 0.3294, 0.3412, 0.3451],\n",
       "          [0.3098, 0.3059, 0.3059,  ..., 0.3137, 0.3255, 0.3373],\n",
       "          [0.2941, 0.2941, 0.3216,  ..., 0.3294, 0.3098, 0.3216]],\n",
       "\n",
       "         [[0.3608, 0.3569, 0.3490,  ..., 0.3412, 0.3529, 0.3647],\n",
       "          [0.3529, 0.3569, 0.3529,  ..., 0.3490, 0.3490, 0.3529],\n",
       "          [0.3569, 0.3647, 0.3647,  ..., 0.3529, 0.3490, 0.3451],\n",
       "          ...,\n",
       "          [0.3176, 0.3255, 0.3216,  ..., 0.3569, 0.3725, 0.3804],\n",
       "          [0.3294, 0.3294, 0.3333,  ..., 0.3333, 0.3529, 0.3686],\n",
       "          [0.3176, 0.3216, 0.3490,  ..., 0.3451, 0.3373, 0.3529]],\n",
       "\n",
       "         [[0.3020, 0.2980, 0.2902,  ..., 0.2863, 0.2941, 0.3098],\n",
       "          [0.2980, 0.3020, 0.2941,  ..., 0.2902, 0.2902, 0.3020],\n",
       "          [0.2980, 0.3098, 0.3098,  ..., 0.2941, 0.2980, 0.2941],\n",
       "          ...,\n",
       "          [0.2706, 0.2784, 0.2627,  ..., 0.2980, 0.3137, 0.3176],\n",
       "          [0.2824, 0.2824, 0.2784,  ..., 0.2824, 0.2980, 0.3098],\n",
       "          [0.2706, 0.2706, 0.2863,  ..., 0.3020, 0.2902, 0.2980]]],\n",
       "\n",
       "\n",
       "        [[[0.5451, 0.4235, 0.3176,  ..., 0.6157, 0.5725, 0.6706],\n",
       "          [0.5490, 0.4627, 0.3059,  ..., 0.5686, 0.5451, 0.5922],\n",
       "          [0.5529, 0.4706, 0.2863,  ..., 0.5020, 0.5176, 0.5412],\n",
       "          ...,\n",
       "          [0.1922, 0.2824, 0.3294,  ..., 0.3255, 0.3412, 0.3882],\n",
       "          [0.2196, 0.2588, 0.2627,  ..., 0.3725, 0.4118, 0.4510],\n",
       "          [0.3294, 0.3490, 0.2863,  ..., 0.4431, 0.4902, 0.5176]],\n",
       "\n",
       "         [[0.4941, 0.3882, 0.2941,  ..., 0.5333, 0.5098, 0.6078],\n",
       "          [0.4824, 0.4157, 0.2863,  ..., 0.4863, 0.4863, 0.5451],\n",
       "          [0.4980, 0.4275, 0.2706,  ..., 0.4353, 0.4667, 0.5020],\n",
       "          ...,\n",
       "          [0.1882, 0.2706, 0.3098,  ..., 0.2941, 0.3098, 0.3490],\n",
       "          [0.2196, 0.2510, 0.2471,  ..., 0.3294, 0.3647, 0.4039],\n",
       "          [0.3098, 0.3255, 0.2627,  ..., 0.3843, 0.4353, 0.4588]],\n",
       "\n",
       "         [[0.4275, 0.3373, 0.2510,  ..., 0.4784, 0.4471, 0.5294],\n",
       "          [0.4314, 0.3569, 0.2392,  ..., 0.4314, 0.4314, 0.4745],\n",
       "          [0.4392, 0.3608, 0.2196,  ..., 0.3882, 0.4235, 0.4549],\n",
       "          ...,\n",
       "          [0.1569, 0.2275, 0.2706,  ..., 0.2706, 0.2745, 0.3098],\n",
       "          [0.1843, 0.2118, 0.2118,  ..., 0.2980, 0.3294, 0.3569],\n",
       "          [0.2549, 0.2706, 0.2235,  ..., 0.3490, 0.3922, 0.4078]]],\n",
       "\n",
       "\n",
       "        [[[0.4667, 0.4235, 0.4431,  ..., 0.3059, 0.3451, 0.3765],\n",
       "          [0.4510, 0.4157, 0.4275,  ..., 0.3333, 0.3490, 0.3529],\n",
       "          [0.4157, 0.4118, 0.4157,  ..., 0.3373, 0.3294, 0.3490],\n",
       "          ...,\n",
       "          [0.7765, 0.7765, 0.7765,  ..., 0.1765, 0.2078, 0.2157],\n",
       "          [0.7176, 0.7216, 0.7255,  ..., 0.2392, 0.2431, 0.2235],\n",
       "          [0.6588, 0.6667, 0.6510,  ..., 0.2431, 0.2471, 0.2314]],\n",
       "\n",
       "         [[0.4275, 0.3922, 0.4039,  ..., 0.2980, 0.3294, 0.3490],\n",
       "          [0.4196, 0.3922, 0.4000,  ..., 0.3176, 0.3294, 0.3294],\n",
       "          [0.4000, 0.3922, 0.3961,  ..., 0.3176, 0.3098, 0.3333],\n",
       "          ...,\n",
       "          [0.6745, 0.6745, 0.6745,  ..., 0.1686, 0.1961, 0.2039],\n",
       "          [0.6275, 0.6314, 0.6275,  ..., 0.2235, 0.2275, 0.2118],\n",
       "          [0.5804, 0.5843, 0.5686,  ..., 0.2275, 0.2314, 0.2196]],\n",
       "\n",
       "         [[0.3333, 0.3059, 0.3098,  ..., 0.2314, 0.2549, 0.2745],\n",
       "          [0.3333, 0.3059, 0.3020,  ..., 0.2549, 0.2588, 0.2549],\n",
       "          [0.3176, 0.3098, 0.3059,  ..., 0.2627, 0.2471, 0.2588],\n",
       "          ...,\n",
       "          [0.5608, 0.5647, 0.5647,  ..., 0.1451, 0.1686, 0.1765],\n",
       "          [0.5059, 0.5098, 0.5137,  ..., 0.1882, 0.1922, 0.1804],\n",
       "          [0.4510, 0.4588, 0.4471,  ..., 0.1882, 0.1922, 0.1843]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = next(iter(train_loader))\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b530c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input.to(device)\n",
    "output = model(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6a6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e187135",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3201df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64])) must be the same as input size (torch.Size([64, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m loss\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/nn/functional.py:3639\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([64])) must be the same as input size (torch.Size([64, 2]))"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(output, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b15467fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bce1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Device: cuda\n",
      "-----------Training starting----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     25\u001b[0m train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[0;32m/home/public/sgurpree/Spring2025/Grad-CV-I_Final-Project/utilities.py:59\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, loss_fn, optimizer, device, train_loader)\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Step the optimizer\u001b[39;00m\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/sgurpree/miniconda3/envs/G-CV-I_Final/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "from utilities import train_step, val_step\n",
    "import pandas as pd\n",
    "train_start_time = timer()\n",
    "\n",
    "print(f\"Training on Device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "train_loss, train_acc = 0, 0\n",
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "# Create two empty dataframes to store loss and accuracy values\n",
    "train_metrics = pd.DataFrame(columns=[\"train_loss\", \"train_acc\"])\n",
    "val_metrics = pd.DataFrame(columns=[\"val_loss\", \"val_acc\"])\n",
    "test_metrics = pd.DataFrame(columns=[\"test_loss\", \"test_acc\"])\n",
    "print(\"-----------Training starting----------\")\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, train_acc = train_step(model, loss_fn, optimizer, device, train_loader)\n",
    "    train_metrics['train_loss'].append(train_loss)\n",
    "    train_metrics['train_acc'].append(train_acc)\n",
    "\n",
    "    val_loss, val_acc, min_val_loss = val_step(model, loss_fn, device, min_val_loss, val_loader)\n",
    "    val_metrics['val_loss'].append(val_loss)\n",
    "    val_metrics['val_acc'].append(val_acc)\n",
    "print(\"-----------Training finished----------\")\n",
    "\n",
    "train_end_time = timer()\n",
    "\n",
    "total_train_time = train_end_time - train_start_time\n",
    "\n",
    "print(f\"Training completed in {total_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import test_step\n",
    "\n",
    "# Testing\n",
    "test_start_time = timer()\n",
    "\n",
    "print(\"-----------Testing starting----------\")\n",
    "\n",
    "test_step(model, loss_fn, device, test_loader)\n",
    "\n",
    "print(\"-----------Testing finished----------\")\n",
    "\n",
    "test_end_time = timer()\n",
    "\n",
    "total_test_time = test_end_time - test_start_time\n",
    "\n",
    "print(f\"Testing completed in {total_test_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G-CV-I_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
